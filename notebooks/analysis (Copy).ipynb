{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4ec8ba-c552-4c2e-94ca-e44008d065f0",
   "metadata": {},
   "source": [
    "# Serbian Parliament NLP analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9169efe4-ec78-43c4-85ce-538c41238c30",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3c278a-8a02-4be4-a218-9da66d37c9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import logging\n",
    "from gensim import corpora, models\n",
    "import stanza\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c6d97d-f80f-4c03-b385-80114b1cb02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/home/vuk/Documents/0 Data Science/serbian_parliament_nlp_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcee7278-ced4-43c0-8cb0-b6483401cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_root = Path().resolve().parent\n",
    "#sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e092c8-9c0b-4e0d-80c0-493bd320b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba9f96b5-9ca8-4233-90ae-12dd7bf3e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7861b989-57ba-4f6f-b767-28ec67af7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 11:56:23 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756974423bd24168949ec97ef12a6516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 11:56:24 INFO: Downloaded file to /home/vuk/stanza_resources/resources.json\n",
      "2025-05-11 11:56:24 INFO: Loading these models for language: sr (Serbian):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | set          |\n",
      "| pos       | set_nocharlm |\n",
      "| lemma     | set_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-05-11 11:56:24 INFO: Using device: cpu\n",
      "2025-05-11 11:56:24 INFO: Loading: tokenize\n",
      "2025-05-11 11:56:24 INFO: Loading: pos\n",
      "2025-05-11 11:56:25 INFO: Loading: lemma\n",
      "2025-05-11 11:56:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Stanza pipeline\n",
    "nlp = stanza.Pipeline(\"sr\", processors=\"tokenize,pos,lemma\", use_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44029e6-28f9-4097-a264-3fb6c86f65af",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "646b36ec-7256-48d8-b9ed-fbad6f9bf237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    try:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        logging.info(f\"Loaded {len(data)} records from {path}\")\n",
    "        return pd.DataFrame(data)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {path}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ce555d-b798-40ae-b772-001335e2438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return set(line.strip().lower() for line in f if line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e14554c5-1670-4078-b8b8-454d573a309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "df = pd.read_json(config.SPEECHES_JSON)\n",
    "stopwords = load_stopwords(config.STOPWORDS_TXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b74121-018d-4936-9210-54e6cf7a1783",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d55424f-739c-44d3-b75b-08ad57c3f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(text, stopwords):\n",
    "    doc = nlp(text)\n",
    "    return [\n",
    "        word.lemma.lower()\n",
    "        for sent in doc.sentences\n",
    "        for word in sent.words\n",
    "        if word.lemma and word.lemma.lower() not in stopwords and len(word.lemma) > 3\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bfd6ad-14e5-4545-9274-17ad89002280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Cleaning\n",
    "df['speech'] = df['speech'].astype(str)\n",
    "df['speaker'] = df['speaker'].astype(str)\n",
    "df['speech_len'] = df['speech'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6ab28-8d01-43d7-989e-4cb90872dd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and Clean\n",
    "df['clean_tokens'] = df['speech'].apply(lambda x: tokenize_and_lemmatize(x, stopwords))\n",
    "df['clean_text'] = df['clean_tokens'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a10fb1-fc04-45b5-9953-ee1fea1d62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(config.LEMMATIZED_JSON, orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5de18-1ca9-4725-81b5-4d3d263ad054",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd51bd-a054-48df-95d5-0b6783af77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(frequencies, title='Most Common Non-Stop Words'):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(frequencies)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2037ba-3c06-41b6-bf59-e460866495fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data, title, xlabel, ylabel, bins=20, xlim=None):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    data.hist(bins=bins, color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    if xlim:\n",
    "        plt.xlim(*xlim)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0ed4f1-9087-4cff-84f9-5299d30a8149",
   "metadata": {},
   "source": [
    "## WordCloud + Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171a12a-2976-4642-aa74-63a0f5af88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Frequency\n",
    "word_freq = Counter()\n",
    "df['clean_tokens'].apply(lambda tokens: word_freq.update(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ced1a2-b9e0-4a7d-8dd0-bd945f0a3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['speech_len'] = df['speech'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e81628-c719-4985-b476-34169e7117b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word cloud\n",
    "wordcloud = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=500,\n",
    "    colormap=\"viridis\"\n",
    ").generate_from_frequencies(word_freq)\n",
    "\n",
    "# Plot and save\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/03 figures/wordcloud_serbia.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dfb794-8b65-4a04-bfbf-6a8abe410394",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.DataFrame(word_freq.items(), columns=['word', 'frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a34012-5645-4e95-9d5e-50fa95b988a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df = pd.DataFrame(word_freq.items(), columns=['word', 'frequency'])\n",
    "\n",
    "# Sort by frequency descending\n",
    "top_words = word_freq_df.sort_values(by='frequency', ascending=False)\n",
    "\n",
    "# Display top 20\n",
    "top_words.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8772c226-58a8-462c-8772-6956832225ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top_words.head(20), x='frequency', y='word', color='skyblue')\n",
    "plt.title(\"Top 20 Most Frequent Words\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/03 figures/top_words_serbia.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6400630-c299-4e21-a4ca-d7aa615d4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_df.to_csv(\"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/0 data/02 interim/word_frequencies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe3ce5-5ca9-441a-9687-d4ef85d91fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'speech_len' not in df.columns:\n",
    "    df['speech_len'] = df['speech'].str.len()\n",
    "\n",
    "# Keep only needed columns (optional)\n",
    "speech_len_df = df[['speaker', 'speech_len']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbcd13-2f68-4350-b663-cc00af660d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_len_df.to_csv(\"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/0 data/02 interim/speech_lengths.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeee6a0-a226-4087-9327-2d506ca13fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(df['speech_len'], 'Distribution of Speech Lengths', 'Speech Length (Characters)', 'Number of Speeches', xlim=(0, 30000))\n",
    "plt.savefig(\"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/03 figures/speech_lengths.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42b612-15ae-4d85-81aa-36ab9aa96c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers_by_count = df['speaker'].value_counts().reset_index()\n",
    "top_speakers_by_count.columns = ['speaker', 'num_speeches']\n",
    "print(top_speakers_by_count.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2dd596-8eea-459d-92d5-9e05724f1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers_by_length = df.groupby('speaker')['speech_len'].sum().sort_values(ascending=False).reset_index()\n",
    "top_speakers_by_length.columns = ['speaker', 'total_speech_len']\n",
    "print(top_speakers_by_length.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7153107b-8d3f-480e-8b2b-1bd5a4835694",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_stats = df.groupby('speaker').agg(\n",
    "    num_speeches=('speech', 'count'),\n",
    "    total_speech_len=('speech_len', 'sum'),\n",
    "    avg_speech_len=('speech_len', 'mean')\n",
    ").sort_values(by='num_speeches', ascending=False).reset_index()\n",
    "\n",
    "print(speaker_stats.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95920c47-7d4b-43be-9052-770c36ef5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10 = speaker_stats.head(15)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top10, x='num_speeches', y='speaker', color='skyblue')\n",
    "plt.title(\"Top 10 Speakers by Number of Speeches\")\n",
    "plt.xlabel(\"Number of Speeches\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/03 figures/top_speakers.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeaf0fe-c53f-4bd1-b77a-69d81c7f2c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 verbose speakers\n",
    "top15_length = top_speakers_by_length.head(15)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=top15_length, x='total_speech_len', y='speaker', color='mediumseagreen')\n",
    "plt.title(\"Top 15 Speakers by Total Speech Length\")\n",
    "plt.xlabel(\"Total Speech Length (Characters or Words)\")\n",
    "plt.ylabel(\"Speaker\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/03 figures/top_speakers_2.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e55f42-5c00-4786-adf3-faba2fc4800d",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6add968-fe3b-469e-a51d-f8095bdb6f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('gensim').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594b47b-4c5d-4c66-9e5c-d7e5e5fa696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(texts, num_topics=10):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lda = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n",
    "    return lda, dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577ca78-2e34-4e5f-8846-3df6afb21459",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model, dictionary, corpus = topic_modeling(df['clean_tokens'].tolist(), NUM_TOPICS)\n",
    "\n",
    "# Print topics\n",
    "for idx, topic in lda_model.print_topics(-1, num_words=10):\n",
    "    print(f\"Topic #{idx}:\")\n",
    "    for term in topic.split(\" + \"):\n",
    "        weight, word = term.split(\"*\")\n",
    "        word_clean = word.strip().strip('\"')\n",
    "        print(f\"  {float(weight):.3f} {word_clean}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a397b6d-9c20-4958-9bf4-39a29246e2bc",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c72e06-818d-47b0-859d-cf6d0c34b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv(\"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/0 data/03 external/serbian_sentiment_latin.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c058e5-648f-40a1-8c5d-b0eb0e546a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = sentiment_df.drop_duplicates(subset=[\"Serbian Word\"])\n",
    "sentiment_df.set_index(\"Serbian Word\", inplace=True)\n",
    "sentiment_dict = sentiment_df.to_dict(orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edced89-2460-4d46-83fc-ef7d19e8f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_scores(tokens, lexicon):\n",
    "    scores = np.zeros(len(next(iter(lexicon.values()))))\n",
    "    for token in tokens:\n",
    "        if token in lexicon:\n",
    "            scores += np.array(list(lexicon[token].values()))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f25e82-c5a0-4e3f-bcf0-abcfa55fc6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "emotion_columns = sentiment_df.columns.tolist()\n",
    "\n",
    "df[emotion_columns] = df['clean_tokens'].apply(lambda tokens: pd.Series(get_emotion_scores(tokens, sentiment_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f05c9-d1a1-43ec-97e2-581c0d181df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with the dominant emotion\n",
    "df['dominant_emotion'] = df[emotion_columns].idxmax(axis=1)\n",
    "\n",
    "# Or just sum total emotion occurrence across all speeches\n",
    "total_emotions = df[emotion_columns].sum().sort_values(ascending=False)\n",
    "print(total_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5d84f-9835-463a-b4cd-fbc8dc93c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_map = pd.read_csv(\"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/0 data/04 metadata/party_map.csv\")\n",
    "df['speaker'] = df['speaker'].str.upper().str.strip()\n",
    "party_map['speaker'] = party_map['speaker'].str.upper().str.strip()\n",
    "\n",
    "df = df.merge(party_map, on='speaker', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4840e-bff0-4cb1-9f7f-f4b8b89db38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_cols = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "total_emotions = df[emotion_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=total_emotions.index, y=total_emotions.values, hue=total_emotions.index, palette=\"viridis\", dodge=False, legend=False)\n",
    "plt.title(\"Total Emotion Counts Across All Speeches\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4eaae0-05a5-4d8e-bb6a-1f33757abaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by party\n",
    "party_emotions = df.groupby('party')[emotion_cols].sum()\n",
    "\n",
    "# Normalize if you want relative proportions\n",
    "party_emotions_norm = party_emotions.div(party_emotions.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40009f88-05c4-43f6-9757-576da7e0f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "party_emotions_long = party_emotions_norm.reset_index().melt(\n",
    "    id_vars='party',\n",
    "    var_name='emotion',\n",
    "    value_name='score'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4dc4db-b376-415b-bacc-25332d5cc4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your save path\n",
    "output_dir = \"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/03 figures\"\n",
    "\n",
    "unique_parties = party_emotions_long['party'].unique()\n",
    "\n",
    "for party in unique_parties:\n",
    "    subset = party_emotions_long[party_emotions_long['party'] == party]\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    ax = sns.barplot(\n",
    "        data=subset,\n",
    "        x='emotion',\n",
    "        y='score',\n",
    "        hue='emotion',\n",
    "        dodge=False,\n",
    "        palette='viridis',\n",
    "        legend=False\n",
    "    )\n",
    "    plt.title(f\"Emotion Proportions â€“ {party}\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, row in enumerate(subset.itertuples()):\n",
    "        ax.text(i, row.score + 0.0005, f\"{row.score:.2f}\", ha='center', va='bottom', fontsize=8)\n",
    "    plt.subplots_adjust(top=0.50)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Sanitize filename\n",
    "    filename = f\"emotions_{party.lower().replace(' ', '_').replace('/', '_')}.png\"\n",
    "    full_path = os.path.join(output_dir, filename)\n",
    "    plt.savefig(full_path, dpi=300)\n",
    "    plt.show()  \n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Saved: {full_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a23419-d749-46fe-a4e8-0c2663005aec",
   "metadata": {},
   "source": [
    "## Summary (Optional Export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074beba-eeb5-46ba-826e-492ec5802646",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2eae9d-7146-4412-9a68-b42bd363700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final DataFrame with clean text and sentiment\n",
    "df[['speaker', 'speech', 'clean_text', 'speech_len', 'positive', 'negative', 'dominant_emotion']].to_csv(\n",
    "    \"/home/vuk/Documents/0 Data Science/parliament_nlp_analysis/0 data/02 interim/cleaned_speeches.csv\",\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
